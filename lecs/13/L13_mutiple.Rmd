
---
title: "STAT340 Lecture 13: Multiple comparison problem"
author: "Keith Levin and Bi Cheng Wu"
date: "October 2021"
output: html_document
knit: (function(inputFile,encoding){rmarkdown::render(
  inputFile,encoding=encoding,output_file=file.path(dirname(inputFile),'index.html'))})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      cache = TRUE, autodep = TRUE, cache.comments = FALSE)
library(Rfast)
library(tidyverse)
```

# What is multiple comparison? <small>(and why is it a problem?)</small>

<center><a href="https://xkcd.com/882/"><img id="comic" src="https://imgs.xkcd.com/comics/significant.png" title="'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'"></a></center>

# People v. Puckett

### Background

The following facts are from a real case ^[http://www.personal.psu.edu/dhk3/dhblog/ROB%28Puckett-CA%29.pdf] ^[https://www.prisonlegalnews.org/news/2009/jan/15/cold-case-hits-use-vastly-exaggerated-dna-match-statistics-8232upheld-by-california-supreme-court/].

On December 22, 1972, around 8:20am, 22-year old Diana Sylvester was found dead in her San Francisco apartment. Her landlord called the police after hearing loud noises, "a violent pounding on the floor", and a woman's screams. Diana was found on the floor unclothed with multiple stab wounds to her chest. She appeared to have been sexually assaulted, then strangled. DNA samples were collected and tested, but no primary suspect was identified and the case went cold for 30 years.

In 2003, the case was reopened, and the samples were reanalyzed and uploaded to the California DNA database ($N\approx338,000$). The initial DNA sample reportedly didn't contain enough identifiable markers to be of use and was extrapolated by an analyst based on "inconclusive" markers to meet the minimum search length requirements. 70-year old John Puckett was identified as a "[cold hit](https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/cold-hit)" from this partial match.

There was no other direct evidence connecting him to the case, so during the trial, prosecution relied heavily on the DNA match, which they claimed based on "expert testimony" had a 1 in 1.1 million chance of matching a random person. Puckett was convicted in 2008 on one count of first degree murder and sentenced to life imprisonment (w/ possibility of parole).

Later, in a paper published in Forensic Science Communications---a peer-reviewed journal published by the Federal Bureau of Investigation (FBI)---researchers pointed out the actual probability of finding a match was closer to 1 in 3 (a fact that judge Benson barred the defense from presenting to the jury during trial) ^[https://archives.fbi.gov/archives/about-us/lab/forensic-science-communications/fsc/july2009/undermicroscope/2009_07_micro01.htm#differentquestions].

So which number is correct? The short answer is they are both right, depending on what question you're asking.

### Different questions, different answers

Assume the DNA analysis expert is right, and the sample had a 1 in 1.1 million probability of matching a random, unrelated, innocent person. In other words, the probability of a false positive is

$$
P(\text{false}\,+)=P(+\,|\,\text{innocent}):=\alpha=\frac1{1.1\times10^6}\approx0.00000091
$$

However, the California DNA database at the time had around $N=338,000$ samples in it. Assume for a moment Diana's aggressor was not in the database (i.e. the samples in the database are all completely unrelated to this case) and that the samples in the database are independent. Then, each sample has $\alpha$ probability of returning a false positive. We can then easily calculate the probability of ***at least one match***

\begin{aligned}
P(\text{at least 1 match})&=1-P(\text{no matches})\\
&=1-\prod_{i=1}^NP(i\text{-th sample is NOT a match})\\
&=1-\prod_{i=1}^N\Big(1-P(\text{$i$-th sample IS a match})\Big)\\
&=1-(1-\alpha)^N\\
&=1-\left(1-\tfrac1{1,100,000}\right)^{338,000}\\
&\approx0.26455\\
&\approx\tfrac1{3.78}
\end{aligned}

In other words, the actual probability of at least one match in the database was about 26\%, or about 1 in 3.8 chance. If the defense was allowed to present this to the jury, would Puckett still have been convicted? We will never know.

The problem here lies in multiple testing. By uploading the sample to the database, detectives were not just running 1 test against a database, but $338,000$ individual tests. Any single, individual sample has a 1 in 1.1 million chance of returning a false positive match, but the entire database has a 26\% chance of returning a false positive match.

[**Yikes!**](https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy)

### Tangent on [replication crisis](https://en.wikipedia.org/wiki/Replication_crisis)

This was an extreme example, but the same principle applies whenever you perform many *post-hoc* tests (i.e. not a few specific, redetermined questions) to search for any significant results without taking into consideration the problem of multiple testing. This is often called [data dredging](https://en.wikipedia.org/wiki/Data_dredging) or data fishing, and is an example of a type of $p$-hacking---a broad term referring to any kind of misused of statistical methods to produce exaggerated significance of results (e.g. $p$-values). This is motivated by the fact that more significant results are traditionally more likely to be published (which researchers and PIs often feel heavily pressured to do).

In recent years, there has been a slowly growing awareness of these issues in the scientific community. In 2005, John Ioannidis published the highly influential paper [Why Most Published Research Findings Are False](https://doi.org/10.1371/journal.pmed.0020124), in which he argued that "most claimed research findings are false" due to many factors, including "high rate of nonreplication (lack of confirmation)"; bias in design, data, analysis, or presentation; selective reporting by editors/publishers of journals; effect sizes often being small compared to the power of the statistical method; and even intentional manipulation and misreporting of results.

Indeed, in 2015, a [large replication study](https://doi.org/10.1126/science.aac4716) of papers from prestigious psychology journals found about 63% of the significant results could NOT be reproduced. In 2016, a [survey of 1,576 researchers](https://doi.org/10.1038/533452a) across many fields found that 70\% "tried and failed to reproduce another scientist's experiments", and more than half "failed to reproduce their own experiments". In 2019, a [replication study of hydrology journals](https://www.nature.com/articles/sdata201930) found with 95\% confidence the proportion of reproducible papers in the sample of nearly 2000 articles was between 0.6\% and 6.8\%.

# Simulation study

Getting back to the topic of multiple comparisons, let's further illustrate the point by running a simulation study. Suppose we draw $g$ groups of $n$ observations ***all from the same normal distribution*** ^[the distribution type doesn't really matter here, but normal was chosen to keep the testing procedure simple, since it's not the focus here]. Suppose we are genuinely curious if any of the groups are different, so we do the naive thing: we conduct $g\choose2$ pairwise $t$-tests all at the 95\% confidence level.

The following script tests this in an automated way, and is able to determine empirically the probability of a false positive in **one of the tests** (i.e. a significant result (even though none of them should be!)).

```{r}
library(Rfast)
library(tidyverse)

mult.comp.sim = function(g,n,i=5000,alpha=0.05){
  sapply(g,function(..){
    replicate(i,ttests.pairs(matrix(rnorm(..*n),ncol=..,nrow=n))$pvalue %>% 
                {.[lower.tri(.)]} %>% {any(.<alpha)}) %>% mean
    })
}

# brief check that it's working
mult.comp.sim(2,20)

# choose some arbitrary n, let's say 20
# we can then show the probability for any given
# g of getting a false positive in at least ONE of the pairwise tests
2:30 %>% 
  {data.frame(g=.,p=mult.comp.sim(.,20),value="data.frame")} %>% 
  ggplot(aes(x=g,y=p)) + geom_point() + geom_smooth(se=F) + 
  geom_hline(aes(yintercept=0.05),linetype="dashed",color="red") + 
  scale_y_continuous(limits=c(0,1),expand=c(0,0)) + 
  labs(title=expression(paste("Probability of false positive in ",
                              bgroup("(",atop(g,2),")")," tests")))
```

As you can see, the problem is quite severe, with the probability of false positive going completely out of control as the number of tests we need to perform increases.

How do we prevent this from happening?

# Bonferroni

One of the most popular and broadly applicable ways is called the Bonferroni correction, named after the [Bonferroni inequalities](https://en.wikipedia.org/wiki/Boole%27s_inequality#Bonferroni_inequalities) that it makes use of in its derivation. It controls what's called the **family-wise error rate** (FWER)---i.e. the probability that at least ONE test in the family results in a false positive.

Suppose we must conduct $m$ tests, and we wish to control the overall FWER at $\alpha$. Let $E_i$ represent the event that the $i$-th test results in a false positive. Suppose we perform each individual test at some more strict significance level $\alpha^*$. How small do we need to make $\alpha^*$ such that $\text{FWER}\leq\alpha$? We solve

$$
\text{FWER}=P\left(\bigcup_{i=1}^mE_i\right)\leq\sum_{i=1}^mP(E_i)=m\alpha^*
$$

Note if we choose $\alpha^*=\frac\alpha m$, then we have

$$
\text{FWER}\leq m\alpha^*\leq\alpha
$$

Thus, the Bonferroni method says if we perform each test at the significance level $\alpha/m$ instead of $\alpha$, then the overall FWER is controlled at $\alpha$.

Let's use this and revisit the earlier simulation example, this time running each test at $\alpha/{g\choose2}$ level and see what the results look like.

```{r}
mult.comp.sim.bonf = function(g,n,i=5000,alpha=0.05){
  sapply(g,function(..){
    replicate(i,ttests.pairs(matrix(rnorm(..*n),ncol=..,nrow=n))$pvalue %>% 
                {.[lower.tri(.)]} %>% {any(.<(alpha/choose(..,2)))}) %>% mean
    })
}

2:30 %>% 
  {data.frame(g=.,p=mult.comp.sim.bonf(.,20),value="data.frame")} %>% 
  ggplot(aes(x=g,y=p)) + geom_point() + geom_smooth(se=F) + 
  geom_hline(aes(yintercept=0.05),linetype="dashed",color="red") + 
  scale_y_continuous(limits=c(0,1),expand=c(0,0)) + 
  labs(title=expression(paste("Probability of false positive in ",
                              bgroup("(",atop(g,2),")")," tests with Bonferroni correction")))
```

Wow that looks so much better! Some more comments about Bonferroni

- Advantages:
  - easy to use
  - very good at controlling FWER
  - easy to apply to any test (just lower $\alpha$)
- Disadvantages:
  - slightly conservative
  - may have lower power than other methods

There are some other methods that are sometimes used, but basically speaking they all make the following tradeoff

|                  |                          |                   |
|-----------------:|:------------------------:|:------------------|
|   more liberal   |                          | more conservative |
|   higher power   |$\Leftarrow===\Rightarrow$|    lower power    |
|lower FWER control|                          |higher FWER control|

Another way of saying this is you can't control both the type I error rate (false positive) and the type II error rate (false negative) at the same time. If you want to control type I, your test literally needs to produce more negative results, which necessarily raises the type II error rate, which lowers power. Conversely, if you want higher power, your test literally needs to produce more positive results, which necessarily raises the type I error rate.

TODO for Wednesday: touch on alternative methods, add more examples, maybe touch on FDR,...



<br/><br/><br/><br/><br/><br/><br/><br/>
